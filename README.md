![Comparison of Baselines with Proposed Model changes](https://user-images.githubusercontent.com/99614234/190555982-bc6789fd-ac3d-45fe-ad33-980a6f1b0cb3.PNG)

# LDAM-DRW-and-RCE
Robust CNN model using LDAM DRW and RCE Loss functions
Here we incorporate the combination of Loss functions LDAM-DRW and Reversed Cross Entropy Techniques to design Robust CNN models for noisy and imbalanced data 

Introduction-
DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes (“easy” classes), but more surprisingly, it also suffers from significant under learning on some other classes (“hard” classes). CE requires an extra term to facilitate learning of hard classes, which should be noise intolerant.
Paper proposes introduction of a noise robust Reverse Cross Entropy term to reduce overfitting of easy classes and under learning of hard classes
Ref [28, 1] state that DNNs first memorize data for clean labels, then for noisy labels. Eg. in ref [13,19].
Cross Entropy causes class-biased learning ie. Easy classes converge faster than hard classes. Under learning of hard classes: a barrier to overall accuracy.
Previous works propose roughly 3 categories of solutions- label correction methods, loss correction methods, refined training strategies.
Challenges of previous works
Label correction- require extra data with clean labels, complex
Loss correction- requires noise transition matrix which is not always available in practice
Refined strats - either require training of an auxiliary n/w or rely on complex interventions into the learning process. Difficult to tune.
Analysis of CE under noisy labels.
Generating symmetric noisy labels- randomly flip a correct label to one of the other (K-1) incorrect labels uniformly. Portion of incorrect labels = noise rate.
In early stages, learning is highly class-biased for both clean and noisy dataset. For clean labels, eventually all classes are learned uniformly whereas for noisy labels, class-wise accuracy varies greatly even in later stages. Easy classes start to overfit in middle stages of learning. Hard classes accuracy significantly lower compared to clean labels.
From Symm KL divergence to Symm CE-
KL divergence K(q||p) is difference of cross-entropy (of ground truth distribution q(k|x) and predicted distribution p(k|x)) and entropy (of ground truth). Classification done by trying to obtain p(k|x) so that KL is minimum. 
Since for noisy labels, q(k|x) does not represent true class distribution, we consider KL divergence from other direction as well K(p||q) and get symmetric KL divergence as K(q||p) + K(p||q).
Adopting that to CE, Symm CE Learning = CE + RCE
SL loss = alpha*Loss(CE) + beta*Loss(RCE)
Alpha – address overfitting
Beta- improve flexibility of RCE’s robustness

“As the ground truth distribution q(kjx) is now inside of the logarithm in Loss(rce), this could cause computational problem when labels are one-hot: zero values inside the logarithm. To solve this issue, we define log 0 = A (where A < 0 is some constant)”  clipping operation.
Generating noise-
“Symmetric noisy labels are generated by flipping the labels of a given proportion of training samples to one of the other class labels uniformly. Whilst for asymmetric noisy labels, flipping labels only occurs within a specific set of classes [16, 29], for example, for MNIST, flipping 2  7, 3  8,      5 <->  6 and 7  1;”   = generating symmetric and asymmetric noise
Setup- 
8 layer CNN: 6 conv layers and 2 fully connected layers. 120 epochs. Stochastic Gradient Descent with momentum 0.9 and initial learning rate = 0.01 which is redivided by 10 after 40 and 80 epochs. 
Alpha= 0.1, beta= 1, A= -6
Learning-
SL facilitates adaptive learning rate to improve learning for hard classes. Decrease learning rate for classes with probab > 0.5 and increase learning rate for classes with probab < 0.5
Incorporating SL into other models
For methods that use robust loss functions or label corrections, the RCE term of SL can be directly added to the loss function, while for methods that still use the standard CE loss without label corrections, SL can be used with small alpha and large beta to replace the existing loss function.













